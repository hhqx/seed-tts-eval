import os
import json
import torch
import torch.multiprocessing as mp
import numpy as np
from tqdm import tqdm
from typing import List, Callable, Dict, Optional
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from evaluate import load
from py3_tools.py_debug import breakpoint

import string
from jiwer import compute_measures, wer
from zhon.hanzi import punctuation

punctuation_all = punctuation + string.punctuation

def wer_seedtts(predictions, references, lang="en"):
    if isinstance(predictions, list):
        assert isinstance(references, list) and len(predictions) == len(references)
        return sum([wer_seedtts(pred, ref, lang) for pred, ref in zip(predictions, references)]) / len(predictions)
    
    truth, hypo = references, predictions
    breakpoint()
    
    if not predictions:
        return None  # å¦‚æœæ²¡æœ‰å‚è€ƒæ–‡æœ¬ï¼Œè¿”å› None

    for x in punctuation_all:
        if x == '\'':
            continue
        truth = truth.replace(x, '')
        hypo = hypo.replace(x, '')

    truth = truth.replace('  ', ' ')
    hypo = hypo.replace('  ', ' ')

    if lang == "zh":
        truth = " ".join([x for x in truth])
        hypo = " ".join([x for x in hypo])
    elif lang == "en":
        truth = truth.lower()
        hypo = hypo.lower()
    else:
        raise NotImplementedError

    measures = compute_measures(truth, hypo)
    wer = measures["wer"]
    breakpoint()
    return wer

# åŠ è½½WERè¯„ä¼°æŒ‡æ ‡
wer = load("wer")
def wer_huggingface(predictions, references):
    """ä½¿ç”¨Hugging Faceçš„werè®¡ç®—"""
    return wer.compute(
        predictions=predictions,
        references=references
    )


def process_example(example):
    """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œè®¡ç®—WER"""
    example["wav_asr_text"] = example["wav_asr_text"].strip() if example["wav_asr_text"] else ""
    example["text"] = example["text"].strip() if example["text"] else ""
    
    example["wer"] = wer_huggingface(
        predictions=[example["wav_asr_text"]],
        references=[example["text"]]
    )
    example["wer"] = wer_seedtts(
        predictions=[example["wav_asr_text"]],
        references=[example["text"]]
    )
    return example

import pandas as pd 
def cal_wer(
    # json_file: str = 'outputs/seedtts-eval/spark-tts/LLM_ad7dbf22/seedtts/wer_asr_results.jsonl',
    json_file: str = 'outputs/seedtts-eval/spark-tts/exp0719_4090x6_constan_lr5e5_perbs256_ds2_6epoch_6417a43a/seedtts/wer_asr_results.jsonl',
    ):
    df = pd.read_json(json_file, lines=True)
    
    df['wer'] = df.apply(lambda x: wer_seedtts(x['wav_asr_text'], x['text']), axis=1)
    
    print(f"WER Mean: {df['wer'].mean()}")
    
# cal_wer()
# breakpoint()


def load_pipe(rank, model_id="openai/whisper-large-v3"):
    from transformers import WhisperProcessor, WhisperForConditionalGeneration 

    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() else "cpu")
    # processor = AutoProcessor.from_pretrained(model_id)
    # model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)
    # model.to(device)
    
    # # åˆ›å»ºpipeline
    # pipe = pipeline(
    #     "automatic-speech-recognition",
    #     model=model,
    #     tokenizer=processor.tokenizer,
    #     feature_extractor=processor.feature_extractor,
    #     device=device,
    #     chunk_length_s=30
    # )

    # model_id = "openai/whisper-large-v3"
    processor = WhisperProcessor.from_pretrained(model_id)
    model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)
    
    
    return model, processor

import soundfile as sf
import scipy
def do_asr(pipe, example):
    model, processor = pipe
    device = next(model.parameters()).device

    wav, sr = sf.read(example["wav_path"])
    if sr != 16000:
        wav = scipy.signal.resample(wav, int(len(wav) * 16000 / sr))
    input_features = processor(wav, sampling_rate=16000, return_tensors="pt").input_features
    input_features = input_features.to(device)
    forced_decoder_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")
    predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription.strip()

def whisper_asr_worker(pipe, example):
    """ASRå¤„ç†å‡½æ•°ï¼Œæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åˆå§‹åŒ–æ¨¡å‹"""
    # åˆå§‹åŒ–å¤„ç†å™¨å’Œæ¨¡å‹
    example['text'] = example.pop('text')
    
    # æ‰§è¡ŒASRæ¨ç†
    try:

        asr_txt = do_asr(pipe, example)
        example["wav_asr_text"] = asr_txt
    except Exception as e:
        example["wav_asr_text"] = None
        example["error"] = str(e)
        print(f"Error processing {example['wav_path']}: {str(e)}")
        
    # è®¡ç®—WERï¼ˆå¦‚æœASRæˆåŠŸï¼‰
    if example["wav_asr_text"] is not None:
        example = process_example(example)
    else:
        example["wer"] = None  # æ ‡è®°é”™è¯¯æ ·æœ¬çš„WER
    
    return example

def main_worker(rank, args):
    """æ¯ä¸ªè¿›ç¨‹çš„ä¸»å‡½æ•°"""
    data, gpu_ids, extra_args, results, errors, lock, counter = args
    ngpus = len(gpu_ids)
    device = torch.device(f"cuda:{rank}")  # rankå¯¹åº”GPU ID
    
    pipe = load_pipe(rank)
    
    # æ•°æ®åˆ†ç‰‡ï¼šæ¯ä¸ªGPUå¤„ç† data[rank::ngpus]
    local_data = data[rank::ngpus]
    print(f"Rank {rank} å¼€å§‹å¤„ç† {len(local_data)} ä¸ªæ ·æœ¬")
    
    # å¤„ç†æœ¬åœ°æ•°æ®
    for idx, item in enumerate(tqdm(local_data, desc=f"Rank {rank} è¿›åº¦")):
        # åŸå§‹æ•°æ®ç´¢å¼•ï¼ˆç”¨äºç»“æœæ’åºï¼‰
        original_idx = item["original_idx"]
        try:
            # è°ƒç”¨ASRå¤„ç†å‡½æ•°
            result = whisper_asr_worker(pipe, item)
            # ä¿å­˜ç»“æœï¼ˆå¸¦åŸå§‹ç´¢å¼•ï¼‰
            with lock:
                results[original_idx] = result
                counter.value += 1
        except Exception as e:
            import traceback
            traceback.print_exc()
            # è®°å½•é”™è¯¯
            with lock:
                print("rank", rank, "error:", str(e))
                errors.append((original_idx, item, str(e)))
    
    print(f"Rank {rank} å¤„ç†å®Œæˆ")


def run_multi_gpu_tasks(
    data: List,
    worker_fn: Callable,
    gpu_ids: List[int],
    extra_args: Dict = {},
    output_file: Optional[str] = None,
    error_file: Optional[str] = None,
    desc: str = "ä»»åŠ¡è¿›åº¦"
):
    # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ åŸå§‹ç´¢å¼•ï¼Œç”¨äºç»“æœæ’åº
    data_with_idx = [{"original_idx": i,** item} for i, item in enumerate(data)]
    total = len(data)
    
    # åˆå§‹åŒ–å…±äº«å†…å­˜ï¼šç»“æœåˆ—è¡¨ï¼ˆé¢„åˆ†é…ç©ºé—´ï¼‰ã€é”™è¯¯åˆ—è¡¨ã€è®¡æ•°å™¨
    mp.set_start_method("spawn", force=True)
    manager = mp.Manager()
    results = manager.list([None] * total)  # å›ºå®šé•¿åº¦ï¼ŒæŒ‰åŸå§‹ç´¢å¼•å­˜å‚¨
    errors = manager.list()
    counter = manager.Value('i', 0)
    lock = manager.Lock()
    
    # å‡†å¤‡ä¼ ç»™æ¯ä¸ªworkerçš„å‚æ•°
    args = (
        data_with_idx,
        gpu_ids,
        extra_args,
        results,
        errors,
        lock,
        counter
    )
    # main_worker(0, args)  # ä¸»è¿›ç¨‹å…ˆæ‰§è¡Œä¸€æ¬¡ï¼ŒåŠ è½½æ¨¡å‹ç­‰
    
    if 1 or not os.path.exists(output_file):
        # å¯åŠ¨å¤šè¿›ç¨‹ï¼ˆæ¯ä¸ªGPUä¸€ä¸ªè¿›ç¨‹ï¼‰
        try:
            # ä½¿ç”¨spawnå¯åŠ¨è¿›ç¨‹ï¼Œnprocs=len(gpu_ids)ï¼Œæ¯ä¸ªè¿›ç¨‹å¯¹åº”ä¸€ä¸ªrankï¼ˆ0åˆ°ngpus-1ï¼‰
            torch.multiprocessing.spawn(
                main_worker,
                args=(args,),
                nprocs=len(gpu_ids),
                join=True
            )
        except KeyboardInterrupt:
            print("ğŸ›‘ æ‰‹åŠ¨ä¸­æ–­")
            raise
        except Exception as e:
            print(f"âŒ è¿›ç¨‹é”™è¯¯: {str(e)}")
            raise
    
        # è½¬æ¢å…±äº«åˆ—è¡¨ä¸ºæ™®é€šåˆ—è¡¨
        results = list(results)
        errors = list(errors)
    else:
        results = pd.read_json(output_file, lines=True).to_dict(orient="records")
        # error_file = pd.read_json(error_file, lines=True).to_dict(orient="records")
        
        for _ in results:
            if np.isnan(_["wer"]):
                _["wer"] = None

    print("âœ… æ±‡æ€»ç»“æœ...")
    
    breakpoint()
    
    # ä¿å­˜è¾“å‡ºæ–‡ä»¶
    if output_file:
        results.sort(key=lambda x: -x["wer"] if x["wer"] is not None else float('inf'))  # æŒ‰WERæ’åº

        pd.DataFrame(results).to_csv(output_file.replace(".jsonl", ".csv"), index=False, header=True)

        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, "w") as f:
            for res in results:
                if res is not None:
                    f.write(json.dumps(res, ensure_ascii=False) + "\n")
    
    # ä¿å­˜é”™è¯¯æ–‡ä»¶
    # if error_file and errors:
    #     os.makedirs(os.path.dirname(error_file), exist_ok=True)
    #     with open(error_file, "w") as f:
    #         for e in errors:
    #             f.write(json.dumps({
    #                 "original_idx": e[0],
    #                 "item": e[1],
    #                 "error": e[2]
    #             }, ensure_ascii=False) + "\n")
    
    # è®¡ç®—æœ‰æ•ˆæ ·æœ¬çš„WERå‡å€¼
    
    valid_wer = [res["wer"] for res in results if res is not None and res["wer"] is not None]
    print(f"âœ… æˆåŠŸ: {len(valid_wer)}/{total}")
    print(f"âŒ å¤±è´¥: {len(errors)}")
    if valid_wer:
        print(f"wer mean: {np.mean(valid_wer)}")
    
    return results, errors

def main(data_folder):
    import glob
    import json
    import numpy as np
    
    # # # sparktts, has_ref, result: 0.0733, seedtts-eval: 0.0253
    # # '/home/hqx/llm-tts/outputs/seedtts-eval/spark-tts/LLM_ad7dbf22',
    
    # # pretrain, one-shot, has_ref, result: 0.0843, seedtts-eval: 0.0314
    # '/home/hqx/llm-tts/outputs/seedtts-eval/spark-tts/exp0719_4090x6_constan_lr5e5_perbs256_ds2_6epoch_6417a43a',
    
    # # Pretrainï¼Œzero-shot, no_ref, result: xxx, seedtts-eval: 0.0298
    # '/home/hqx/llm-tts/outputs/seedtts-eval/spark-tts/exp0719_4090x6_constan_lr5e5_perbs256_ds2_6epoch_1853c44b',
    
    
    # # Original repo, one-shot, has_ref, result: xxx, seedtts-eval: 0.0244
    # '/home/hqx/Spark-TTS/example/seedtts-results',
    
    # # Original repo, zero-shot, no_ref, result: xxx, seedtts-eval: xxx
    # '/home/hqx/Spark-TTS/example/seedtts-results-male_moderate_moderate',

    
    output_folder = os.path.join(data_folder, "seedtts")
    os.makedirs(output_folder, exist_ok=True)
    
    json_files = glob.glob(os.path.join(data_folder, "text_wav_*.json"))
    if len(json_files) != 1:
        raise ValueError(f"Expected exactly one JSON file in {data_folder}, found {len(json_files)}")
    input_json = json_files[0]
    output_file = os.path.join(output_folder, "wer_asr_results.jsonl")
    error_file = os.path.join(output_folder, "wer_asr_failed_samples.jsonl")
    
    print(f"åŠ è½½æ•°æ®é›†: {input_json}")
    with open(input_json) as f:
        data = json.load(f)
    
    # è¿è¡Œå¤šGPUä»»åŠ¡

    ngpu = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {ngpu} ä¸ªå¯ç”¨GPU")
    results, errors = run_multi_gpu_tasks(
        data=data,
        worker_fn=whisper_asr_worker,
        # gpu_ids=[0],  # ä½¿ç”¨çš„GPU IDåˆ—è¡¨
        gpu_ids=list(range(ngpu)),  # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰GPU
        extra_args={"model_id": "openai/whisper-large-v3"},
        output_file=output_file,
        error_file=error_file,
        desc="Whisper å¤šGPU ASRæ¨ç†"
    )
    
    # è®¡ç®—å¹¶æ‰“å°å¹³å‡WERï¼ˆè¿‡æ»¤é”™è¯¯æ ·æœ¬ï¼‰
    valid_wer = [res["wer"] for res in results if res is not None and res["wer"] is not None]
    if valid_wer:
        print(f"æœ€ç»ˆWERå‡å€¼: {np.mean(valid_wer):.4f}")
    else:
        print("æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬è®¡ç®—WER")
    
    with open(output_file.replace('.jsonl', '.log'), "w") as f:
        f.write(f"æœ€ç»ˆWERå‡å€¼: {np.mean(valid_wer):.4f}\n")
        f.write(f"æˆåŠŸæ ·æœ¬æ•°: {len(valid_wer)}\n")
        f.write(f"å¤±è´¥æ ·æœ¬æ•°: {sum(res['wer'] is None for res in results)}\n")


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    for data_folder in [
        # # sparktts, one-shot, has_ref,  seedtts-eval: 0.0246
        '/home/hqx/llm-tts/outputs/seedtts-eval/spark-tts/LLM_ad7dbf22',
        
        # # pretrain, one-shot, has_ref, seedtts-eval: 0.0314
        # '/home/hqx/llm-tts/outputs/seedtts-eval/spark-tts/exp0719_4090x6_constan_lr5e5_perbs256_ds2_6epoch_6417a43a',
        
        # # Pretrainï¼Œzero-shot, no_ref, seedtts-eval: 0.0298
        # '/home/hqx/llm-tts/outputs/seedtts-eval/spark-tts/exp0719_4090x6_constan_lr5e5_perbs256_ds2_6epoch_1853c44b',
        
        
        # # Original repo, one-shot, has_ref,  seedtts-eval: 0.0244
        # '/home/hqx/Spark-TTS/example/seedtts-results',
        
        # # Original repo, zero-shot, no_ref,  seedtts-eval: 0.0249
        # '/home/hqx/Spark-TTS/example/seedtts-results-male_moderate_moderate',
    ]:
        print(f"å¤„ç†æ•°æ®æ–‡ä»¶å¤¹: {data_folder}")
        main(data_folder)
 